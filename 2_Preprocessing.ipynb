{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b008d44c",
   "metadata": {},
   "source": [
    "# `Preprocessing workflow`\n",
    "\n",
    "#### The preprocessing workflow consists of ten steps:\n",
    "\n",
    "![Preprocessing.png](images/Preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f20767",
   "metadata": {},
   "source": [
    "At the end of the workflow, you can delete the interim directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d77bcb",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyopenms import *\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "import matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da76ec1a",
   "metadata": {},
   "source": [
    "#### `1) PrecursorCorrection` (To the \"highest intensity MS1 peak\")\n",
    "\n",
    "This algorithm is used directly after the file introduction, in order to correct any wrong MS1 precursor annotation. For each MS2 spectrum the corresponding MS1 spectrum is determined by using the RT information of the precursor. In the MS1 spectrum, the highest intensity peak is selected as the corrected precursor. We assume that, in a given mass window  (e.g. precursor mass +/- 10 ppm), the precursor with the hightest intensity was actually fragmented (top-n method), which is a method used in the Thermo Orbitrap instrument (Center for Biosustainability).\n",
    "\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/release/latest/html//classOpenMS_1_1PrecursorCorrection.html#a8acf85ba8b9f249de0369bb083355982 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"results\"\n",
    "if not os.path.exists(results):\n",
    "    os.mkdir(results)\n",
    "\n",
    "interim = os.path.join(results, \"interim\")\n",
    "if not os.path.exists(interim):\n",
    "    os.mkdir(interim)\n",
    "\n",
    "path = os.path.join(interim, \"mzML\")\n",
    "\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "else:\n",
    "    os.mkdir(path)\n",
    "\n",
    "\n",
    "input_original_files = glob.glob(os.path.join(\"data\", \"mzML\", \"*.mzML\"))\n",
    "\n",
    "for filename in input_original_files:\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "    exp.sortSpectra(True)\n",
    "    delta_mzs = []\n",
    "    mzs = []\n",
    "    rts = []\n",
    "    PrecursorCorrection.correctToHighestIntensityMS1Peak(exp, 100.0, True, delta_mzs, mzs, rts)\n",
    "    mzmlfile_path = os.path.join(path, \"PCpeak_\" + os.path.basename(filename))\n",
    "    MzMLFile().store(mzmlfile_path, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac01d6",
   "metadata": {},
   "source": [
    "#### `2) Mass trace detection`\n",
    "\n",
    "A mass trace extraction method that gathers peaks similar in m/z and moving along retention time.\n",
    "Peaks of a MSExperiment are sorted by their intensity and stored in a list of potential chromatographic apex positions. Only peaks that are above the noise threshold (user-defined) are analyzed and only peaks that are n times above this minimal threshold are considered as apices. This saves computational resources and decreases the noise in the resulting output.\n",
    "Starting with these, mass traces are extended in- and decreasingly in retention time. During this extension phase, the centroid m/z is computed on-line as an intensity-weighted mean of peaks.\n",
    "The extension phase ends when either the frequency of gathered peaks drops below a threshold (min_sample_rate, see MassTraceDetection parameters) or when the number of missed scans exceeds a threshold (trace_termination_outliers, see MassTraceDetection parameters).\n",
    "Finally, only mass traces that pass a filter (a certain minimal and maximal length as well as having the minimal sample rate criterion fulfilled) get added to the result.\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/classOpenMS_1_1MassTraceDetection.html#abff6e392ce6da7af8f083397494a7971\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749854bf",
   "metadata": {},
   "source": [
    "#### `3) Elution peak detection`\n",
    "\n",
    "Extracts chromatographic peaks from a mass trace.\n",
    "Mass traces may consist of several consecutively (partly overlapping) eluting peaks, e.g., stemming from (almost) isobaric compounds that are separated by retention time. Especially in metabolomics, isomeric compounds with exactly the same mass but different retentional behaviour may still be contained in the same mass trace. This method first applies smoothing on the mass trace\"s intensities, then detects local minima/maxima in order to separate the chromatographic peaks from each other. Depending on the \"width_filtering\" parameters, mass traces are filtered by length in seconds (\"fixed\" filter) or by quantile.\n",
    "\n",
    "This method is in other words \"deconvolution\".\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/classOpenMS_1_1ElutionPeakDetection.htmldetails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1e4ca",
   "metadata": {},
   "source": [
    "#### `4) Feature detection`\n",
    "\n",
    "FeatureFinderMetabo assembles metabolite features from singleton mass traces.\n",
    "Mass traces alone would allow for further analysis such as metabolite ID or statistical evaluation. However, in general, monoisotopic mass traces are accompanied by satellite C13 peaks and thus may render the analysis more difficult. FeatureFinderMetabo fulfills a further data reduction step by assembling compatible mass traces to metabolite features (that is, all mass traces originating from one metabolite). To this end, multiple metabolite hypotheses are formulated and scored according to how well differences in RT (optional), m/z or intensity ratios match to those of theoretical isotope patterns.\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/classOpenMS_1_1FeatureFindingMetabo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b538e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(interim, \"Preprocessing\")\n",
    "\n",
    "if not os.path.exists(path): # if it doesn't exist\n",
    "    os.mkdir(path) # create a path directory\n",
    "else:\n",
    "    shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "\n",
    "input_mzml_files = glob.glob(os.path.join(interim, \"mzML\", \"PCpeak_*.mzML\"))\n",
    "\n",
    "# 2) Mass trace detection\n",
    "\n",
    "feature_maps_FFM= []\n",
    "for filename in input_mzml_files:\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "    mass_traces = []\n",
    "    mtd = MassTraceDetection()\n",
    "    mtd_par = mtd.getDefaults()\n",
    "    mtd_par.setValue(\"mass_error_ppm\", 10.0) \n",
    "    mtd_par.setValue(\"noise_threshold_int\", 1.0e02)\n",
    "    mtd.setParameters(mtd_par)\n",
    "    mtd.run(exp, mass_traces, 0)\n",
    "\n",
    "# 3) Elution peak detection (deconvolution)\n",
    "\n",
    "    mass_traces_split = []\n",
    "    mass_traces_final = []\n",
    "    epd = ElutionPeakDetection()\n",
    "    epd_par = epd.getDefaults()\n",
    "    epd_par.setValue(\"width_filtering\", \"fixed\")\n",
    "    epd.setParameters(epd_par)\n",
    "    epd.detectPeaks(mass_traces, mass_traces_split)\n",
    "     \n",
    "    if (epd.getParameters().getValue(\"width_filtering\") == \"auto\"):\n",
    "          epd.filterByPeakWidth(mass_traces_split, mass_traces_final)\n",
    "    else:\n",
    "          mass_traces_final = mass_traces_split\n",
    "\n",
    "# 4) Feature finding metabo (isotope reduction)\n",
    "  \n",
    "    feature_map_FFM = FeatureMap()\n",
    "    feat_chrom = []\n",
    "    ffm = FeatureFindingMetabo()\n",
    "    ffm_par = ffm.getDefaults() \n",
    "    ffm_par.setValue(\"isotope_filtering_model\", \"none\")\n",
    "    ffm_par.setValue(\"remove_single_traces\", \"true\")\n",
    "    ffm_par.setValue(\"report_convex_hulls\", \"true\")\n",
    "    ffm.setParameters(ffm_par)\n",
    "    ffm.run(mass_traces_final, feature_map_FFM, feat_chrom)\n",
    "    \n",
    "    feature_map_FFM.setUniqueIds()\n",
    "    feature_map_FFM.setPrimaryMSRunPath([filename.encode()])  \n",
    "\n",
    "    feature_maps_FFM.append(feature_map_FFM)\n",
    "    featurefile = os.path.join(path, \"FFM_\" + os.path.basename(filename)[7:-5] +\".featureXML\")\n",
    "    FeatureXMLFile().store(featurefile, feature_map_FFM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffec84",
   "metadata": {},
   "source": [
    "#### `5) PrecursorCorrection (To the \"nearest feature”)`\n",
    "\n",
    "This algorithm is used after feature detection to allow for precursor correction on MS2 level. \n",
    "\n",
    "If there are MS2 spectra in the feature space which have been measured in isotope traces, it “corrects” the MS2 spectrum annotation to the monoisotopic trace. \n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/classOpenMS_1_1PrecursorCorrection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(interim, \"mzML\")\n",
    "\n",
    "input_mzml_files = glob.glob(os.path.join(interim, \"mzML\", \"PCpeak_*.mzML\"))\n",
    "input_feature_files = glob.glob(os.path.join(interim, \"Preprocessing\", \"FFM_*.featureXML\")) # or \"FFM_*.featureXML\" if you haven't filtered the blanks/control/QC\n",
    "\n",
    "for mzml in input_mzml_files:\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(mzml, exp)\n",
    "    correct = PrecursorCorrection()\n",
    "\n",
    "    for filename in input_feature_files:\n",
    "        feature_map_MFD = FeatureMap()\n",
    "        FeatureXMLFile().load(filename, feature_map_MFD)\n",
    "        if os.path.basename(mzml)[7:-5] == os.path.basename(filename)[9:-11]:\n",
    "            correct.correctToNearestFeature(feature_map_MFD, exp, 0.0, 100.0, True, False, False, False, 3, 0)\n",
    "            corrected_file = os.path.join(path, \"PCfeature_\" + os.path.basename(mzml)[7:])\n",
    "            MzMLFile().store(corrected_file, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1160110",
   "metadata": {},
   "source": [
    "#### `6) MapAlignerPoseClustering `\n",
    "This algorithm is used to perform a linear retention time alignment, in order to correct for chromatographic shifts in retention time. The reference file used for Map Alignment is the feature map with the highest number of features.\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/TOPP_MapAlignerPoseClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(interim, \"Preprocessing\")\n",
    "interim= os.path.join(\"results\", \"interim\")\n",
    "\n",
    "input_feature_files = glob.glob(os.path.join(path, \"FFM_*.featureXML\"))\n",
    "feature_maps=[]\n",
    "for filename in input_feature_files:\n",
    "    feature_map_MFD = FeatureMap()\n",
    "    FeatureXMLFile().load(filename, feature_map_MFD)\n",
    "    feature_maps.append(feature_map_MFD)\n",
    "\n",
    "ref_index = [i[0] for i in sorted(enumerate([fm.size() for fm in feature_maps]), key=lambda x:x[1])][-1]\n",
    "\n",
    "aligner = MapAlignmentAlgorithmPoseClustering()\n",
    "aligner_par= aligner.getDefaults()\n",
    "\n",
    "aligner_par.setValue(\"max_num_peaks_considered\", -1)\n",
    "aligner_par.setValue(\"superimposer:mz_pair_max_distance\", 0.05)\n",
    "aligner_par.setValue(\"pairfinder:distance_MZ:max_difference\", 10.0)\n",
    "aligner_par.setValue(\"pairfinder:distance_MZ:unit\", \"ppm\")\n",
    "aligner.setParameters(aligner_par)\n",
    "aligner.setReference(feature_maps[ref_index])\n",
    "\n",
    "for feature_map in feature_maps[:ref_index] + feature_maps[ref_index+1:]:\n",
    "    trafo = TransformationDescription()\n",
    "    aligner.align(feature_map, trafo)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "    transformer.transformRetentionTimes(feature_map, trafo, True) # store original RT as meta value\n",
    "\n",
    "for feature_map in feature_maps:    \n",
    "    feature_file = os.path.join(path, \"MapAligned_\" + os.path.basename(feature_map.getMetaValue(\"spectra_data\")[0].decode())[7:-5] +\".featureXML\")\n",
    "    trafo_file= os.path.join(path, \"MapAligned_\" + os.path.basename(feature_map.getMetaValue(\"spectra_data\")[0].decode())[7:-5] +\".trafoXML\")\n",
    "    FeatureXMLFile().store(feature_file, feature_map)\n",
    "    TransformationXMLFile().store(trafo_file, trafo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca273d1",
   "metadata": {},
   "source": [
    "Visualisation of data before and after alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36177759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_maps = [feature_maps[ref_index]] + feature_maps[:ref_index] + feature_maps[ref_index+1:]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"consensus map before alignment\")\n",
    "ax.set_ylabel(\"m/z\")\n",
    "ax.set_xlabel(\"RT\")\n",
    "\n",
    "# use alpha value to display feature intensity\n",
    "ax.scatter([f.getRT() for f in feature_maps[0]], [f.getMZ() for f in feature_maps[0]],\n",
    "            alpha = np.asarray([f.getIntensity() for f in feature_maps[0]])/max([f.getIntensity() for f in feature_maps[0]]))\n",
    "\n",
    "for fm in feature_maps[1:]:\n",
    "    ax.scatter([f.getMetaValue(\"original_RT\") for f in fm], [f.getMZ() for f in fm],\n",
    "                alpha = np.asarray([f.getIntensity() for f in fm])/max([f.getIntensity() for f in fm]))\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.set_title(\"consensus map after alignment\")\n",
    "ax.set_xlabel(\"RT\")\n",
    "\n",
    "for fm in feature_maps:\n",
    "    ax.scatter([f.getRT() for f in fm], [f.getMZ() for f in fm],\n",
    "                alpha = np.asarray([f.getIntensity() for f in fm])/max([f.getIntensity() for f in fm]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf28f7",
   "metadata": {},
   "source": [
    "#### `7) MapAlignmentTransformer`\n",
    "This algorithm is used to perform a linear retention time alignment, in order to correct for chromatographic shifts in retention time. Use the trafo XML files from the feature alignment and align the raw spectra.\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/classOpenMS_1_1MapAlignmentTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mzml_files = sorted(glob.glob(os.path.join(interim, \"mzML\", \"PCpeak_*.mzML\")))\n",
    "input_trafo = sorted(glob.glob(os.path.join(interim, \"Preprocessing\", \"*.trafoXML\")))\n",
    "\n",
    "for filename in input_mzml_files:\n",
    "    exp= MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "    exp.sortSpectra(True)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "\n",
    "    for trafo_XML in input_trafo:\n",
    "        trafo=TransformationDescription()\n",
    "        TransformationXMLFile().load(trafo_XML, trafo, True)\n",
    "\n",
    "        if os.path.basename(trafo_XML)[11:-9] == os.path.basename(filename)[7:-5]:\n",
    "            transformer.transformRetentionTimes(exp, trafo, True)\n",
    "            mzml_file = os.path.join(interim, \"mzML\", \"MapAligned_\" + os.path.basename(exp.getLoadedFilePath())[7:-5] +\".mzML\")\n",
    "            MzMLFile().store(mzml_file, exp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e89743b7",
   "metadata": {},
   "source": [
    "#### `8) Metabolite adduct decharger` \n",
    "\n",
    "For each peak, this algorithm reconstructs neutral masses by enumerating all possible adducts with matching charge. Here, we do not save the file with neutral masses, but only the feature files that contain adduct annotations. You can add the list of adduct for the algorithm to parse through. SIRIUS, an algorithm that is later used, is only able to compute singly charged adducts so charges higher than 1 are filtered out. Use adduct list: [b\"H-1:-:1\", b\"H-2O-1:0:0.05\", b\"CH2O2:0:0.5\"] for negative mode, copy-pasting the following script:\n",
    "```\n",
    "input_feature_files= sorted(glob.glob(os.path.join(path, \"MapAligned_*.featureXML\")))\n",
    "\n",
    "for filename in input_feature_files:\n",
    "        feature_map = FeatureMap()    \n",
    "        FeatureXMLFile().load(filename, feature_map)\n",
    "        mfd = MetaboliteFeatureDeconvolution()\n",
    "        mdf_par = mfd.getDefaults()\n",
    "        mdf_par.setValue(\"negative_mode\", \"true\")\n",
    "        mdf_par.setValue(\"potential_adducts\", [b\"H-1:-:1\", b\"H-2O-1:0:0.5\", b\"CH2O2:0:0.5\"])\n",
    "        mdf_par.setValue(\"charge_min\", -2, \"Minimal possible charge\")\n",
    "        mdf_par.setValue(\"charge_max\", 0, \"Maximal possible charge\")\n",
    "        mdf_par.setValue(\"charge_span_max\", 2)\n",
    "        mdf_par.setValue(\"max_neutrals\", 1)\n",
    "        mdf_par.setValue(\"retention_max_diff\", 3.0)\n",
    "        mdf_par.setValue(\"retention_max_diff_local\", 3.0)\n",
    "        mfd.setParameters(mdf_par)\n",
    "        feature_map_MFD = FeatureMap()\n",
    "        cons_map0 = ConsensusMap()\n",
    "        cons_map1 = ConsensusMap()\n",
    "        mfd.compute(feature_map, feature_map_MFD, cons_map0, cons_map1)\n",
    "        featurefile = os.path.join(path, \"MFD_\" + os.path.basename(filename)[11:])\n",
    "        FeatureXMLFile().store(featurefile, feature_map_MFD)\n",
    "```\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/UTILS_MetaboliteAdductDecharger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_files= sorted(glob.glob(os.path.join(path, \"MapAligned_*.featureXML\")))\n",
    "\n",
    "for filename in input_feature_files:\n",
    "        feature_map = FeatureMap()    \n",
    "        FeatureXMLFile().load(filename, feature_map)\n",
    "        mfd = MetaboliteFeatureDeconvolution()\n",
    "        mdf_par = mfd.getDefaults()\n",
    "        mdf_par.setValue(\"potential_adducts\", [b\"H:+:0.4\",b\"Na:+:0.2\",b\"NH4:+:0.2\", b\"H-1O-1:+:0.1\", b\"H-3O-2:+:0.1\"])\n",
    "        mdf_par.setValue(\"charge_min\", 1, \"Minimal possible charge\")\n",
    "        mdf_par.setValue(\"charge_max\", 1, \"Maximal possible charge\")\n",
    "        mdf_par.setValue(\"charge_span_max\", 1)\n",
    "        mdf_par.setValue(\"max_neutrals\", 1)\n",
    "        mdf_par.setValue(\"retention_max_diff\", 3.0)\n",
    "        mdf_par.setValue(\"retention_max_diff_local\", 3.0)\n",
    "        mfd.setParameters(mdf_par)\n",
    "        feature_map_MFD = FeatureMap()\n",
    "        cons_map0 = ConsensusMap()\n",
    "        cons_map1 = ConsensusMap()\n",
    "        mfd.compute(feature_map, feature_map_MFD, cons_map0, cons_map1)\n",
    "        featurefile = os.path.join(path, \"MFD_\" + os.path.basename(filename)[11:])\n",
    "        FeatureXMLFile().store(featurefile, feature_map_MFD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e064bf",
   "metadata": {},
   "source": [
    "#### `9) IDMapper` \n",
    "\n",
    "Introduce the features to a protein identification file (idXML)- the only way to annotate MS2 spectra for GNPS FBMN  (of later importance)\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/nightly/html/TOPP_IDMapper.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f532bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_files = sorted(glob.glob(os.path.join(path, \"MFD_*.featureXML\")))\n",
    "\n",
    "feature_maps = []\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap)\n",
    "    feature_maps.append(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_centroid_rt= False\n",
    "use_centroid_mz= True\n",
    "protein_ids = []\n",
    "peptide_ids= []\n",
    "\n",
    "mapper = IDMapper()\n",
    "input_mzml_files= sorted(glob.glob(os.path.join(\"results\", \"interim\", \"mzML\", \"MapAligned_*.mzML\")))\n",
    "\n",
    "for filename in input_mzml_files:\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "    for fmap in feature_maps:\n",
    "        peptide_ids = []\n",
    "        protein_ids = []\n",
    "        if os.path.basename(fmap.getMetaValue(\"spectra_data\")[0].decode())[7:] == os.path.basename(filename)[11:]:\n",
    "            mapper.annotate(fmap, peptide_ids, protein_ids, use_centroid_rt, use_centroid_mz, exp)\n",
    "        featureidx_file = os.path.join(path, \"IDMapper_\" + os.path.basename(fmap.getMetaValue(\"spectra_data\")[0].decode())[7:-4] +\"featureXML\")\n",
    "        FeatureXMLFile().store(featureidx_file, fmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bb87e",
   "metadata": {},
   "source": [
    "#### `10) FeatureGroupingAlgorithmKD `\n",
    "\n",
    "Feature linker clusters the feature information (from single files) into a ConsensusFeature, linking features from different files together, which have a smiliar m/z and rt (no MS2 data).\n",
    "\n",
    "###### Documentation: https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/TOPP_FeatureLinkerUnlabeledKD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_files = glob.glob(os.path.join(interim, \"Preprocessing\", \"IDMapper_*.featureXML\"))\n",
    "feature_maps = []\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap)\n",
    "    feature_maps.append(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_grouper = FeatureGroupingAlgorithmKD()\n",
    "\n",
    "consensus_map = ConsensusMap()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = os.path.basename(feature_map.getMetaValue(\"spectra_data\")[0].decode())[7:]\n",
    "    file_description.size = feature_map.size()\n",
    "    file_descriptions[i] = file_description\n",
    "\n",
    "feature_grouper.group(feature_maps, consensus_map)\n",
    "consensus_map.setUniqueIds()\n",
    "consensus_map.setColumnHeaders(file_descriptions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3afb8662",
   "metadata": {},
   "source": [
    "Optionally filter consensus features with too many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frac = 0.0 # minimum fraction of samples to be considered a valid consensus feature\n",
    "# e.g. 0.5 -> values from at least 50% of samples required\n",
    "\n",
    "# take consensus map from feature linking step as input...\n",
    "print(f\"Number of consensus features before filtering with threshold of {min_frac}: {consensus_map.size()}\")\n",
    "\n",
    "cm_filtered = ConsensusMap(consensus_map)\n",
    "cm_filtered.clear(False)\n",
    "\n",
    "n_samples = len(consensus_map.getColumnHeaders())\n",
    "\n",
    "for cf in consensus_map:\n",
    "    if len(cf.getFeatureList()) / n_samples >= min_frac:\n",
    "        cm_filtered.push_back(cf)\n",
    "\n",
    "print(f\"Number of consensus features after filtering: {cm_filtered.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "664c0a85",
   "metadata": {},
   "source": [
    "Optionally filter blank/QC/control consensus features with a mean blank intensity / mean sample intensity ration above a cutoff value (e.g. 30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b653e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of blank files\n",
    "blank_files = []\n",
    "# Define the intensity cutoff\n",
    "cutoff = 0.3\n",
    "\n",
    "print(f\"Number of consensus features before blank filtering: {cm_filtered.size()}\")\n",
    "\n",
    "# Create a new ConsensusMap as a copy of cm\n",
    "cm_blank_filtered = ConsensusMap(cm_filtered)\n",
    "\n",
    "# Remove all the features but keep meta data\n",
    "cm_blank_filtered.clear(False)\n",
    "\n",
    "# Get a dict with map indeces and file names\n",
    "headers = cm_filtered.getColumnHeaders()\n",
    "\n",
    "# Get map indeces for blank files\n",
    "blank_indices = []\n",
    "for index, value in headers.items():\n",
    "    if value.filename in blank_files:\n",
    "        blank_indices.append(index)\n",
    "\n",
    "# Iterate over the consensus features in the original consensus map\n",
    "for cf in cm_filtered:\n",
    "    # Get dict with index and intensity pairs from each feature in the consensus feature\n",
    "    features = {f.getMapIndex(): f for f in cf.getFeatureList()}\n",
    "\n",
    "    # Compute the average intensity in the blank files\n",
    "    blank_intensities = [\n",
    "        value.getIntensity() for key, value in features.items() if key in blank_indices\n",
    "    ]\n",
    "    if blank_intensities:\n",
    "        blank_mean = np.mean(blank_intensities)\n",
    "    else:\n",
    "        blank_mean = 1\n",
    "\n",
    "    # Compute the average intensity in the non-blank files\n",
    "    non_blank_intensities = [\n",
    "        value.getIntensity() for key, value in features.items() if key not in blank_indices\n",
    "    ]\n",
    "    if non_blank_intensities:\n",
    "        non_blank_mean = np.mean(non_blank_intensities)\n",
    "    else:\n",
    "        non_blank_mean = 1  # set 1 to avoid division by zero error in the ratio\n",
    "\n",
    "    # Compute the ratio between the two intensities (add one to avoid division by zero error)\n",
    "    ratio = blank_mean / non_blank_mean\n",
    "\n",
    "    # If the ratio is smaller than the cutoff, add the consensus feature to the new consensus map\n",
    "    if ratio < cutoff:\n",
    "        new_cf = ConsensusFeature(cf)\n",
    "        new_cf.clear()\n",
    "        for index, feature in features.items():\n",
    "            if index in blank_indices:\n",
    "                continue\n",
    "            bf = BaseFeature()\n",
    "            bf.setRT(feature.getRT())\n",
    "            bf.setMZ(feature.getMZ())\n",
    "            bf.setIntensity(feature.getIntensity())\n",
    "            bf.setUniqueId(feature.getUniqueId())\n",
    "            new_cf.insert(index, bf)\n",
    "        cm_blank_filtered.push_back(new_cf)\n",
    "\n",
    "# Set new column headers without blank files\n",
    "cm_blank_filtered.setColumnHeaders({index: header for index, header in headers.items() if index not in blank_indices})\n",
    "\n",
    "print(f\"Number of consensus features after blank filtering: {cm_blank_filtered.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "096d8930",
   "metadata": {},
   "source": [
    "Save ConsensusMap to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ede273",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consensus_file= os.path.join(path, \"consensus.consensusXML\")\n",
    "ConsensusXMLFile().store(Consensus_file, cm_blank_filtered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2ed0e5",
   "metadata": {},
   "source": [
    "Get consensus feature intensities as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_map = ConsensusMap()\n",
    "ConsensusXMLFile().load(Consensus_file, consensus_map)\n",
    "\n",
    "df = consensus_map.get_df()\n",
    "for cf in consensus_map:\n",
    "    if cf.metaValueExists(\"best ion\"):\n",
    "        df[\"adduct\"] = [cf.getMetaValue(\"best ion\") for cf in consensus_map]\n",
    "        break\n",
    "\n",
    "df= df.reset_index()\n",
    "df= df.drop(columns= [\"sequence\"])\n",
    "df= df.rename(columns={\"RT\": \"RT(s)\", \"mz\" :\"m/z\"})\n",
    "\n",
    "# annotate original feature IDs\n",
    "fnames = [Path(value.filename).name for value in consensus_map.getColumnHeaders().values()]\n",
    "\n",
    "ids = [[] for _ in fnames]\n",
    "\n",
    "for cf in consensus_map:\n",
    "    fids = {f.getMapIndex(): f.getUniqueId() for f in cf.getFeatureList()}\n",
    "    for i, fname in enumerate(fnames):\n",
    "        if i in fids.keys():\n",
    "            ids[i].append(str(fids[i]))\n",
    "        else:\n",
    "            ids[i].append(pd.NA)\n",
    "\n",
    "for i, f in enumerate(fnames):\n",
    "    df[f\"{fnames[i]}_IDs\"] = ids[i]\n",
    "\n",
    "feature_dir = os.path.join(\"results\", \"features\")\n",
    "if not os.path.exists(feature_dir):\n",
    "    os.mkdir(feature_dir)\n",
    "\n",
    "# store as tsv file\n",
    "df.to_csv(os.path.join(feature_dir, \"FeatureMatrix.tsv\"), sep = \"\\t\", index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214dcf8f",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"RT(s)\", y=\"m/z\", color=\"quality\")\n",
    "fig.update_layout(title=\"Consensus features\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ed466-e57e-4ad6-be4d-8ab42a3ec183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "af38ad5e49aed933f8d6ee223f599c93f64b7c643a606a4150c2ce501c9413ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
